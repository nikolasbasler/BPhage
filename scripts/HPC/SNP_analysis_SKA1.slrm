#!/bin/bash -l
#SBATCH --job-name="SNP_analysis_SKA1"
#SBATCH --cluster=genius
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=12
#SBATCH --mem-per-cpu=4G
#SBATCH --time=1:00:00
#SBATCH --account=lp_jm_virome_group
#SBATCH --output=slurm-%x_%a_%A.out

###############################################################################

# Submit (from output/slurm_log)
# sbatch --array=1-150 --export=ALL ../../scripts/HPC/SNP_analysis_SKA1.slrm

conda activate viper_bphage
repo_location="$VSC_STAGING/BPhage"
threads=12

line=$(head -n $SLURM_ARRAY_TASK_ID $repo_location/data/BPhage.bee.pool.list | tail -n1)

mkdir -p $VSC_SCRATCH/BPhage/SNP/ska/skf_files
mkdir -p $VSC_SCRATCH/BPhage/SNP/merged_reads
cd $VSC_SCRATCH/BPhage/SNP/

# Re-create the reads that mapped against the bee genome (were deleted during ViPER pipeline)
for gutpart in $(echo "mid ile rec"); do 
    for pair in $(echo "R1 R2"); do
        zcat $repo_location/output/bphage_viper_output/READ/${line}_${gutpart}_*.Hostout.${pair}.fastq.gz | \
            awk 'NR%4==1 {print $0}' | sed 's/^@//g' > temp.${line}_${gutpart}.Hostout.${pair}.readnames
        seqkit grep -v -f temp.${line}_${gutpart}.Hostout.${pair}.readnames \
            $repo_location/output/bphage_viper_output/READ/${line}_${gutpart}_*.trimmed.${pair}.fastq.gz | \
            pigz -p $threads > temp.${line}_${gutpart}.bee.${pair}.fastq.gz
    done
done
# Merge reads per bee pool 
cat temp.${line}_*.bee.R1.fastq.gz > merged_reads/${line}.bee.R1.fastq.gz
cat temp.${line}_*.bee.R2.fastq.gz > merged_reads/${line}.bee.R2.fastq.gz

# Get reads mapping to all phages
samtools merge -@ $threads - $repo_location/output/mapped_phages/${line}*.bam | \
    samtools view -@ $threads -b -F4 | \
    samtools collate -@ $threads -O - | \
    samtools fastq -1 merged_reads/${line}.phages.R1.fastq.gz -2 merged_reads/${line}.phages.R2.fastq.gz \
    -0 temp.${line}.phages.unpaired.reads.fastq.gz -s temp.${line}.phages.singleton.reads.fastq.gz

# Merge Hostout reads per bee pool
cat $repo_location/output/bphage_viper_output/READ/${line}*Hostout.R1.fastq.gz > temp.${line}.Hostout.R1.fastq.gz
cat $repo_location/output/bphage_viper_output/READ/${line}*Hostout.R2.fastq.gz > temp.${line}.Hostout.R2.fastq.gz

echo "#####################################################"
echo "#####################################################"
echo "#####################################################"

datasets=$(echo -e "\
bee
phages
bacteria\
")

conda activate ska
for dataset in $(echo "$datasets"); do
    ska fastq -o ska/skf_files/${line}_${dataset} merged_reads/${line}.${dataset}.R1.fastq.gz merged_reads/${line}.${dataset}.R2.fastq.gz
    
    # For diagnosis. Should not yield too many variant sites per sample because we are running under a haploid assumption.
    # Meaning that only one allele per site will be recorded. If there are too many sites with variants surviving the filters
    # (e.g. frequency >0.2) then a random one will be picked (the last one that the code sees, in fact, so not necessarily the major allele). 
    ska humanise -i ska/skf_files/${line}_${dataset}.skf -o ska/skf_files/${line}_${dataset}.kmers
    sed 1d ska/skf_files/${line}_${dataset}.kmers.tsv | cut -f1 | sort | uniq -c | sort -r -n -k1 > ska/skf_files/${line}_${dataset}.kmercounts

done

# Clean up
rm temp.${line}*

echo "========================================================================"
duration=$SECONDS
printf 'Job finished in: %02d:%02d:%02d\n' $((duration/3600)) $((duration%3600/60)) $((duration%60))